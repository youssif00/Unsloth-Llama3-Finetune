{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "1gR48s4JMgnm"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## The FLASH ATTENTION Bug\n",
        "**Run these two cells if you faced the error about the flash attention when you try to train the model. **"
      ],
      "metadata": {
        "id": "hHiKux2QEWjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Must be set BEFORE importing transformers/accelerate\n",
        "os.environ[\"USE_FLASH_ATTENTION\"] = \"0\"\n",
        "os.environ[\"DISABLE_FLASH_ATTN\"] = \"1\"\n",
        "os.environ[\"TRITON_DISABLE_LINE_INFO\"] = \"1\"\n",
        "\n",
        "import torch\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "print(\"USE_FLASH_ATTENTION:\", os.environ.get(\"USE_FLASH_ATTENTION\"))\n",
        "print(\"DISABLE_FLASH_ATTN:\", os.environ.get(\"DISABLE_FLASH_ATTN\"))\n"
      ],
      "metadata": {
        "id": "iny-SI7JLpGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, os\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
        "print(\"USE_FLASH_ATTENTION:\", os.environ.get(\"USE_FLASH_ATTENTION\"))\n",
        "print(\"DISABLE_FLASH_ATTN:\", os.environ.get(\"DISABLE_FLASH_ATTN\"))\n"
      ],
      "metadata": {
        "id": "yuHjQSc8LIDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Importing"
      ],
      "metadata": {
        "id": "cGWOKpBH1YNZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02ObyzhG05v4"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers datasets trl unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth.chat_templates import get_chat_template, standardize_sharegpt"
      ],
      "metadata": {
        "id": "VErKH96U1S4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Model"
      ],
      "metadata": {
        "id": "4O_H51WV2uue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**  Explain the LORA**\n",
        "\n",
        "** Core Concept: LoRA (Low-Rank Adaptation)\n",
        "Imagine one of the large weight matrices inside the model, let's call it W. A full fine-tune would calculate an update matrix, ΔW, and change the original weights to W + ΔW. This ΔW matrix is the same large size as W, which is why it's so memory-intensive.\n",
        "\n",
        "\n",
        "The key insight of LoRA is that the update matrix ΔW can be effectively approximated by multiplying two much smaller, \"low-rank\" matrices.\n",
        "\n",
        "ΔW ≈ B * A\n",
        "If W is a 4096 x 4096 matrix, ΔW would also be 4096 x 4096 (containing ~16.7 million parameters).\n",
        "With LoRA, we can replace it with two smaller matrices:\n",
        "Matrix A of size 4096 x 16\n",
        "Matrix B of size 16 x 4096\n",
        "The total number of parameters in A and B is (4096 * 16) + (16 * 4096) = 131,072, which is a ~99.2% reduction compared to training the full ΔW matrix.\n",
        "\n",
        "During training, the original weight matrix W is frozen (not updated), and only the new, small matrices A and B are trained. This is the core of how PEFT/LoRA saves so much memory and computation. **"
      ],
      "metadata": {
        "id": "KXuZBCAhPLbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length=2048, load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Add LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model, r=16,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "XxLoGi_-1-9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataSet"
      ],
      "metadata": {
        "id": "fH9IwQ884Eqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.1\")\n",
        "dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "dataset = dataset.map(\n",
        "    lambda examples: {\n",
        "        \"text\": [\n",
        "            tokenizer.apply_chat_template(convo, tokenize=False)\n",
        "            for convo in examples[\"conversations\"]\n",
        "        ]\n",
        "    },\n",
        "    batched=True\n",
        ")"
      ],
      "metadata": {
        "id": "p7HylwJ33DiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset\n",
        "print(\"\\nFirst example full structure:\")\n",
        "print(dataset[0:3])"
      ],
      "metadata": {
        "id": "JiqFeExd4IyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Set up"
      ],
      "metadata": {
        "id": "wlGeHILT-Mjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=2048,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=60,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "-yJexwht5VMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trian the Model"
      ],
      "metadata": {
        "id": "Jba2bmLy_sS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "zOwcVNtc-5jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save The Model"
      ],
      "metadata": {
        "id": "5sjWvdBGMTid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the finetuned model\n",
        "model.save_pretrained(\"finetuned_Llama_model\")"
      ],
      "metadata": {
        "id": "A1NKRniW_vqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test The model"
      ],
      "metadata": {
        "id": "1gR48s4JMgnm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JjoFyEG5PsvO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}